{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping protocol descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_link_to_protocol_query(df_protocols):\n",
    "    query_base = 'https://clinicalstudies.info.nih.gov/cgi/cs/processqry2.pl?search1='\n",
    "    query_tail = '&searchtype=e&SearchButton99a=Submit+Query'\n",
    "    results_urls = []\n",
    "    for prot in df_protocols.protocol_id:\n",
    "        results_urls.append(query_base + prot + query_tail)\n",
    "    df_protocols['link_1'] = results_urls\n",
    "    return df_protocols\n",
    "\n",
    "\n",
    "def get_pages_at_link_1(df_protocols):\n",
    "    \"\"\"Return the search query results webpages for each protocol (labeled link 1)\"\"\"\n",
    "    pages_results = []\n",
    "    for link in df_protocols.link_1:\n",
    "        pages_results.append(requests.get(link))\n",
    "    return pages_results\n",
    "\n",
    "\n",
    "def scrape_results_of_queries(page_results, df_protocols):\n",
    "    \"\"\"With the page of results from each link_1, follow the link and extract all important information at the subsequent page\"\"\"\n",
    "    links = []\n",
    "    #from IPython.core.debugger import Tracer\n",
    "    #Tracer()() #this one triggers the debugger\n",
    "    for ii,page in enumerate(page_results):\n",
    "\n",
    "        result_soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        # val = [link_element for  link_element in results_soup.select('html body div p a')][0]\n",
    "        try:\n",
    "            link_2 = result_soup.find_all('img')[2].find('a')['href']  \n",
    "            next_page_soup = BeautifulSoup(requests.get(link_2).content, 'html.parser')\n",
    "            info_text = '\\n'.join(next_page_soup.find_all('img')[2].get_text().splitlines()[:-4])\n",
    "            link_3 = next_page_soup.find_all('a')[2]['href']\n",
    "        except:\n",
    "            link_2, info_text, link_3 = (np.NaN for i in range(3))\n",
    "            print('no protocol by found:' + page.url)\n",
    "        df_protocols.loc[ii,'link_2']  = link_2\n",
    "        df_protocols.loc[ii, 'link_3'] = link_3\n",
    "        df_protocols.loc[ii, 'info_text'] = info_text\n",
    "    \n",
    "    return df_protocols\n",
    "\n",
    "\n",
    "def get_prot_info_at_link_3(s):\n",
    "    \"\"\"Scrape the protocol page at link_3\"\"\"\n",
    "    try:\n",
    "        \n",
    "        protocol_page = requests.get(s.link_3)\n",
    "        df_prot = pd.read_html(protocol_page.text)\n",
    "        s = pd.concat([s,\n",
    "                       pd.Series(data=df_prot[0][1].tolist(),index=df_prot[0][0].tolist()),\n",
    "                       pd.Series(data=df_prot[1].T[1].tolist(),index=df_prot[1].T[0].tolist())],\n",
    "                      axis = 0)\n",
    "    except:\n",
    "        pass\n",
    "    return s\n",
    "\n",
    "\n",
    "def scrape_set_of_protocols(input_protocols_df):\n",
    "    \"\"\"Given a set of protocols (as a dataframe), search and scrape available info at clinical studies.info.nih.gov \"\"\"\n",
    "    page_results = get_results_of_queries(input_protocols_df)\n",
    "    df_scraped = scrape_results_of_queries(page_results, input_protocols_df)\n",
    "    df_scraped = df_scraped.apply(get_prot_info_at_link_3, axis = 1)\n",
    "    return df_scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load protocol dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_and_m = pd.read_csv('./protocols_unaccounted_n_and_m.csv')\n",
    "not_n_and_m = pd.read_csv('./protocols_unaccounted_others.csv')\n",
    "n_and_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Do the scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_and_m_scraped = scrape_set_of_protocols(n_and_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_n_and_m_scraped = scrape_set_of_protocols(not_n_and_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the output\n",
    "\n",
    "For protocols outside NIMH and NINDS we can check the number of protocols queried and the number of filled values for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(not_n_and_m_scraped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_n_and_m_scraped.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For protocols in NIMH and NINDS the number of protocols queried and the number of filled values for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(n_and_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_and_m_scraped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
